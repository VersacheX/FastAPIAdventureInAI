import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from config import CORS_ORIGINS

# Import dependencies and routers
from dependencies import get_db, get_current_user
from routers import auth, users, game_ratings, worlds, deep_memory, tokenized_history, history, saved_games

app = FastAPI()

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,
    allow_credentials=False,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Register routers
app.include_router(auth.router)
app.include_router(users.router)
app.include_router(game_ratings.router)
app.include_router(worlds.router)
app.include_router(deep_memory.router)
app.include_router(tokenized_history.router)
app.include_router(history.router)
app.include_router(saved_games.router)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)

async def list_tokenized_history(
    game_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    game = db.query(SavedGame).filter(SavedGame.id == game_id).first()
    if not game or game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your saved game")
    # Only return active tokenized chunks (not compressed into deep memory)
    return db.query(TokenizedHistory).filter(
        TokenizedHistory.saved_game_id == game_id,
        TokenizedHistory.is_tokenized == 0
    ).all()


# Deep Memory Endpoints
@app.get("/saved_games/{game_id}/deep_memory/")
async def get_deep_memory(
    game_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Returns the ultra-compressed deep memory for ancient history (if it exists).
    """
    game = db.query(SavedGame).filter(SavedGame.id == game_id).first()
    if not game or game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your saved game")
    deep_memory = db.query(DeepMemory).filter(DeepMemory.saved_game_id == game_id).first()
    if not deep_memory:
        return {}  # Return empty object for consistency
    return {
        "id": deep_memory.id,
        "summary": deep_memory.summary,
        "token_count": deep_memory.token_count,
        "chunks_merged": deep_memory.chunks_merged,
        "last_merged_end_index": deep_memory.last_merged_end_index,
        "updated_at": deep_memory.updated_at
    }

@app.post("/deep_memory/")
async def create_deep_memory(
    deep_memory: DeepMemoryCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Create a new DeepMemory entry for a saved game.
    """
    game = db.query(SavedGame).filter(SavedGame.id == deep_memory.saved_game_id).first()
    if not game or game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your saved game")

    # Only allow one DeepMemory per saved_game_id
    existing = db.query(DeepMemory).filter(DeepMemory.saved_game_id == deep_memory.saved_game_id).first()
    if existing:
        raise HTTPException(status_code=400, detail="Deep memory already exists for this saved game")

    token_count = ai_count_tokens(deep_memory.summary)
    new_deep_memory = DeepMemory(
        saved_game_id=deep_memory.saved_game_id,
        summary=deep_memory.summary,
        token_count=token_count,
        chunks_merged=0,
        last_merged_end_index=0,
        updated_at=datetime.utcnow()
    )
    db.add(new_deep_memory)
    db.commit()
    db.refresh(new_deep_memory)
    return {
        "id": new_deep_memory.id,
        "summary": new_deep_memory.summary,
        "token_count": new_deep_memory.token_count,
        "chunks_merged": new_deep_memory.chunks_merged,
        "last_merged_end_index": new_deep_memory.last_merged_end_index,
        "updated_at": new_deep_memory.updated_at
    }

@app.put("/deep_memory/{deep_memory_id}")
async def update_deep_memory(
    deep_memory_id: int,
    update: DeepMemoryUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Update the deep memory summary (for manual editing).
    Recalculates token count automatically.
    """
    deep_memory = db.query(DeepMemory).filter(DeepMemory.id == deep_memory_id).first()
    if not deep_memory:
        raise HTTPException(status_code=404, detail="Deep memory not found")

    # Verify ownership via saved_game
    game = db.query(SavedGame).filter(SavedGame.id == deep_memory.saved_game_id).first()
    if not game or game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your deep memory")

    # Update summary
    deep_memory.summary = update.summary

    # Recalculate token count
    deep_memory.token_count = ai_count_tokens(update.summary)

    # Update timestamp
    deep_memory.updated_at = datetime.utcnow()

    db.commit()
    db.refresh(deep_memory)

    return {
        "id": deep_memory.id,
        "summary": deep_memory.summary,
        "token_count": deep_memory.token_count,
        "chunks_merged": deep_memory.chunks_merged,
        "last_merged_end_index": deep_memory.last_merged_end_index,
        "updated_at": deep_memory.updated_at
    }

@app.get("/saved_games/{game_id}/token_stats")
async def get_token_stats(
    game_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Returns token statistics for the game:
    - active_tokens: Tokens sent to AI (recent MAX_TOKENIZED_HISTORY_BLOCK chunks + up to TOKENIZE_THRESHOLD tokens of recent history)
    - total_tokens: All tokens in all history entries
    """
    game = db.query(SavedGame).filter(SavedGame.id == game_id).first()
    if not game or game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your saved game")
    
    MAX_TOKENIZED_HISTORY_BLOCK = get_setting('MAX_TOKENIZED_HISTORY_BLOCK', db)
    TOKENIZE_THRESHOLD = get_setting('TOKENIZE_THRESHOLD', db)
    
    # Get all history
    all_history = db.query(StoryHistory).filter(
        StoryHistory.saved_game_id == game_id
    ).order_by(StoryHistory.id).all()
    
    # Get tokenized chunks ordered by most recent
    tokenized_chunks = db.query(TokenizedHistory).filter(
        TokenizedHistory.saved_game_id == game_id
    ).order_by(TokenizedHistory.end_index.desc()).limit(MAX_TOKENIZED_HISTORY_BLOCK).all()
    
    # Calculate active history tokens (most recent untokenized entries up to TOKENIZE_THRESHOLD)
    untokenized_history = [h for h in all_history if not h.is_tokenized]
    untokenized_history.reverse()  # Most recent first
    
    active_history_tokens = 0
    active_history_count = 0
    for entry in untokenized_history:
        entry_tokens = entry.token_count or 0
        if active_history_tokens + entry_tokens <= TOKENIZE_THRESHOLD:
            active_history_tokens += entry_tokens
            active_history_count += 1
        else:
            break
    
    # Calculate active tokenized tokens
    active_tokenized_tokens = sum(chunk.token_count or 0 for chunk in tokenized_chunks)
    
    # Total active tokens sent to AI
    active_tokens = active_tokenized_tokens + active_history_tokens
    
    # Calculate total tokens
    total_tokens = sum(h.token_count or 0 for h in all_history)
    
    return {
        "active_tokens": active_tokens,
        "total_tokens": total_tokens,
        "active_tokenized_chunks": len(tokenized_chunks),
        "active_history_entries": active_history_count,
        "total_history_entries": len(all_history)
    }

# Saved games endpoints
@app.put("/saved_games/{game_id}", response_model=SavedGameDTO)
async def update_saved_game(
    game_id: int,
    game_data: SavedGameCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    game = db.query(SavedGame).filter(SavedGame.id == game_id).first()
    if not game:
        raise HTTPException(status_code=404, detail="Saved game not found")
    if game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your saved game")
    # Update fields
    game.world_id = game_data.world_id
    game.rating_id = game_data.rating_id
    game.player_name = game_data.player_name
    game.player_gender = game_data.player_gender
    game.updated_at = datetime.now(timezone.utc)
    db.commit()
    db.refresh(game)

    return game

@app.post("/saved_games/", response_model=SavedGameIdResponse, status_code=201)
async def create_saved_game(
    game_data: SavedGameCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    if game_data.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: user mismatch")
    new_game = SavedGame(
        user_id=game_data.user_id,
        world_id=game_data.world_id,
        rating_id=game_data.rating_id,
        player_name=game_data.player_name,
        player_gender=game_data.player_gender,
        created_at=datetime.now(timezone.utc),
        updated_at=datetime.now(timezone.utc)
    )
    db.add(new_game)
    db.commit()
    db.refresh(new_game)

    # Save history entries if provided
    if game_data.history:
        for idx, entry in enumerate(game_data.history):
            new_history = StoryHistory(
                saved_game_id=new_game.id,
                entry_index=idx,
                text=entry.entry,
                created_at=datetime.now(timezone.utc)
            )
            db.add(new_history)

    # Save tokenized history blocks if provided
    if game_data.tokenized_history:
        for th in game_data.tokenized_history:
            new_th = TokenizedHistory(
                saved_game_id=new_game.id,
                start_index=th.start_index,
                end_index=th.end_index,
                summary=th.summary,
                created_at=datetime.now(timezone.utc)
            )
            db.add(new_th)

    db.commit()
    # Ensure initial history entries get token counts
    check_and_tokenize_history(new_game.id, db, username=current_user.username)
    return {"id": new_game.id}

@app.delete("/saved_games/{game_id}", response_model=dict)
async def delete_saved_game(
    game_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    game = db.query(SavedGame).filter(SavedGame.id == game_id).first()
    if not game:
        raise HTTPException(status_code=404, detail="Saved game not found")
    if game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your saved game")
    db.delete(game)
    db.commit()
    return {"detail": "Saved game deleted"}

@app.get("/history/{saved_game_id}", response_model=List[HistoryDTO])
async def get_history_for_saved_game(
    saved_game_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    game = db.query(SavedGame).filter(SavedGame.id == saved_game_id).first()
    if not game:
        raise HTTPException(status_code=404, detail="Saved game not found")
    if game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your saved game")
    history_entries = db.query(StoryHistory).filter(StoryHistory.saved_game_id == saved_game_id).all()
    return [history_to_dto(h) for h in history_entries]

@app.post("/history/", response_model=HistoryDTO, status_code=201)
async def create_history_entry(
    history_data: HistoryEntryIn,
    saved_game_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    game = db.query(SavedGame).filter(SavedGame.id == saved_game_id).first()
    if not game or game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your saved game")
    max_entry_index = db.query(func.max(StoryHistory.entry_index)).filter(StoryHistory.saved_game_id == saved_game_id).scalar()
    next_entry_index = (max_entry_index or -1) + 1
    new_history = StoryHistory(
        saved_game_id=saved_game_id,
        entry_index=next_entry_index,
        text=history_data.entry,  # <-- CORRECT
        created_at=datetime.now(timezone.utc)
    )
    db.add(new_history)
    db.commit()
    db.refresh(new_history)
    
    # Update game timestamp
    game.updated_at = datetime.now(timezone.utc)
    db.commit()
    
    # TODO: Check if tokenization is needed and queue background task
    # For now, just check synchronously
    check_and_tokenize_history(saved_game_id, db, username=current_user.username)
    
    return history_to_dto(new_history)

def check_and_tokenize_history(saved_game_id: int, db: Session, username: str = None):
    """
    Token-based history compression system:
    - Calculate token counts for all history entries
    - Create tokenized chunks when untokenized entries exceed token limit
    - Mark history entries as tokenized and track references
    - Refine the most recent tokenized chunk when new tokens are added to it
    """
    limits = get_memory_limits(db)
    TOKENIZE_THRESHOLD = get_setting('TOKENIZE_THRESHOLD', db)  # 800 tokens triggers compression
    TOKENIZED_HISTORY_BLOCK_SIZE = get_setting('TOKENIZED_HISTORY_BLOCK_SIZE', db)  # 200 tokens per chunk
    
    # Get all history entries ordered by index
    all_history = db.query(StoryHistory).filter(
        StoryHistory.saved_game_id == saved_game_id
    ).order_by(StoryHistory.entry_index).all()
    
    if not all_history:
        return
    
    # Calculate token counts for entries that don't have them
    entries_needing_counts = [h for h in all_history if h.token_count is None]
    if entries_needing_counts:
        texts = [h.text for h in entries_needing_counts]
        token_counts = count_tokens_batch(texts)
        for entry, count in zip(entries_needing_counts, token_counts):
            entry.token_count = count
        db.commit()
    
    # Get untokenized entries
    untokenized = [h for h in all_history if not h.is_tokenized]
    
    if not untokenized:
        return
    
    # Calculate total tokens in untokenized entries
    total_untokenized_tokens = sum(h.token_count or 0 for h in untokenized)
    
    # Check if we should create a new tokenized chunk (when untokenized exceeds TOKENIZE_THRESHOLD)
    if total_untokenized_tokens >= TOKENIZE_THRESHOLD:
        # Get the most recent tokenized chunk for this game
        latest_tokenized = db.query(TokenizedHistory).filter(
            TokenizedHistory.saved_game_id == saved_game_id
        ).order_by(TokenizedHistory.end_index.desc()).first()
        
        # Check if we should merge with the latest chunk (if it's less than 90% full)
        should_merge = False
        chunk_entries = untokenized
        
        if latest_tokenized and latest_tokenized.token_count:
            # Calculate if the latest chunk is less than 90% of target size
            utilization = latest_tokenized.token_count / TOKENIZED_HISTORY_BLOCK_SIZE
            if utilization < 0.9:
                should_merge = True
                # Get the history entries referenced by the latest chunk
                if latest_tokenized.history_references:
                    ref_ids = [int(id.strip()) for id in latest_tokenized.history_references.split(',')]
                    ref_entries = db.query(StoryHistory).filter(StoryHistory.id.in_(ref_ids)).all()
                    # Merge: old chunk entries + new untokenized entries
                    chunk_entries = ref_entries + untokenized
        
        # Summarize the chunk (either merged or new)
        # Get previous chunk summary for context (not the one we're updating)
        previous_summary = None
        if should_merge:
            # When merging, get the chunk BEFORE the one we're updating
            previous_chunk = db.query(TokenizedHistory).filter(
                TokenizedHistory.saved_game_id == saved_game_id,
                TokenizedHistory.end_index < latest_tokenized.start_index
            ).order_by(TokenizedHistory.end_index.desc()).first()
            if previous_chunk:
                previous_summary = previous_chunk.summary
        else:
            # When creating new chunk, use the latest existing chunk as context
            if latest_tokenized:
                previous_summary = latest_tokenized.summary
        
        chunk_text = [e.text for e in chunk_entries]
        summary = ai_summarize_chunk(chunk_text, TOKENIZED_HISTORY_BLOCK_SIZE, previous_summary=previous_summary, username=username)
        summary_token_count = count_tokens_batch([summary])[0]
        
        # Create history references string
        history_ids = [str(e.id) for e in chunk_entries]
        history_references = ','.join(history_ids)
        
        if should_merge and latest_tokenized:
            # Update existing chunk with merged content
            latest_tokenized.summary = summary
            latest_tokenized.token_count = summary_token_count
            latest_tokenized.end_index = chunk_entries[-1].entry_index
            latest_tokenized.history_references = history_references
            print(f"Updated tokenized chunk (was {utilization*100:.1f}% full, merged with {len(untokenized)} new entries)")
        else:
            # Create new tokenized chunk
            new_tokenized = TokenizedHistory(
                saved_game_id=saved_game_id,
                start_index=chunk_entries[0].entry_index,
                end_index=chunk_entries[-1].entry_index,
                summary=summary,
                token_count=summary_token_count,
                history_references=history_references,
                created_at=datetime.now(timezone.utc)
            )
            db.add(new_tokenized)
            print(f"Created new tokenized chunk with {len(chunk_entries)} entries ({summary_token_count} tokens)")
        
        # Mark all chunk entries as tokenized
        for entry in chunk_entries:
            entry.is_tokenized = 1
        
        db.commit()
        
        # Check if we need to compress into deep memory
        compress_old_chunks_to_deep_memory(saved_game_id, db, username)

def compress_old_chunks_to_deep_memory(saved_game_id: int, db: Session, username: str = None):
    """
    When tokenized chunks exceed MAX_TOKENIZED_HISTORY_BLOCK, merge oldest chunks into deep memory.
    This keeps the tokenized history manageable while preserving ancient story context.
    """
    MAX_TOKENIZED_HISTORY_BLOCK = get_setting('MAX_TOKENIZED_HISTORY_BLOCK', db)
    DEEP_MEMORY_MAX_TOKENS = get_setting('DEEP_MEMORY_MAX_TOKENS', db)
    
    # Count current ACTIVE tokenized chunks (not yet compressed into deep memory)
    chunk_count = db.query(TokenizedHistory).filter(
        TokenizedHistory.saved_game_id == saved_game_id,
        TokenizedHistory.is_tokenized == 0
    ).count()
    
    if chunk_count <= MAX_TOKENIZED_HISTORY_BLOCK:
        return  # No compression needed
    
    # How many chunks to merge into deep memory
    chunks_to_compress = chunk_count - MAX_TOKENIZED_HISTORY_BLOCK + 2  # Compress extras + 2 more
    
    # Get oldest ACTIVE chunks to compress
    old_chunks = db.query(TokenizedHistory).filter(
        TokenizedHistory.saved_game_id == saved_game_id,
        TokenizedHistory.is_tokenized == 0
    ).order_by(TokenizedHistory.end_index.asc()).limit(chunks_to_compress).all()
    
    if not old_chunks:
        return
    
    print(f"\n{'='*80}")
    print(f"DEEP MEMORY COMPRESSION: {len(old_chunks)} chunks exceed limit")
    print(f"{'='*80}")
    print(f"Current tokenized chunks: {chunk_count}")
    print(f"Max allowed: {MAX_TOKENIZED_HISTORY_BLOCK}")
    print(f"Compressing {chunks_to_compress} oldest chunks into deep memory...")
    print(f"{'='*80}\n")
    
    # Get or create deep memory for this game
    deep_memory = db.query(DeepMemory).filter(
        DeepMemory.saved_game_id == saved_game_id
    ).first()
    
    # Combine old chunks with existing deep memory
    summaries_to_merge = []
    if deep_memory:
        summaries_to_merge.append(deep_memory.summary)
    
    for chunk in old_chunks:
        summaries_to_merge.append(chunk.summary)
    
    # Ultra-compress into deep memory
    prompt = (
        "Compress these story summaries into a single ultra-concise deep memory.\n"
        "Extract ONLY the most critical information:\n"
        "  - Major plot arcs and their resolutions\n"
        "  - Significant character introductions and relationship shifts\n"
        "  - World-changing events or discoveries\n"
        "  - Ongoing missions or tasks\n"
        "Remove ALL minor details, scene descriptions, and redundant information.\n"
        "# Summaries to Compress:\n\n"
        + "\n\n---\n\n".join(summaries_to_merge)
    )
    
    # Use AI to compress (reuse summarize endpoint with custom prompt)    
    # Pass the combined summaries as a single chunk
    deep_summary = ai_summarize_chunk([prompt], max_tokens=DEEP_MEMORY_MAX_TOKENS, previous_summary=None, username=username)
    deep_token_count = count_tokens_batch([deep_summary])[0]
    
    if deep_memory:
        # Update existing deep memory
        deep_memory.summary = deep_summary
        deep_memory.token_count = deep_token_count
        deep_memory.chunks_merged += len(old_chunks)
        deep_memory.last_merged_end_index = old_chunks[-1].end_index
        deep_memory.updated_at = datetime.now(timezone.utc)
        print(f"Updated deep memory: {deep_memory.chunks_merged} total chunks compressed")
    else:
        # Create new deep memory
        deep_memory = DeepMemory(
            saved_game_id=saved_game_id,
            summary=deep_summary,
            token_count=deep_token_count,
            chunks_merged=len(old_chunks),
            last_merged_end_index=old_chunks[-1].end_index,
            created_at=datetime.now(timezone.utc)
        )
        db.add(deep_memory)
        print(f"Created deep memory: {len(old_chunks)} chunks compressed")
    
    # Mark the compressed tokenized chunks as tokenized (compressed into deep memory)
    for chunk in old_chunks:
        chunk.is_tokenized = 1
    
    db.commit()
    
    # Calculate and display total token budget
    TOKENIZE_THRESHOLD = get_setting('TOKENIZE_THRESHOLD', db)
    remaining_chunks = chunk_count - chunks_to_compress
    total_memory_tokens = deep_token_count + (remaining_chunks * get_setting('TOKENIZED_HISTORY_BLOCK_SIZE', db)) + TOKENIZE_THRESHOLD
    
    print(f"âœ“ Deep memory compression complete!")
    print(f"  - Deep Memory: {deep_token_count} tokens")
    print(f"  - Tokenized Chunks ({remaining_chunks}): {remaining_chunks * get_setting('TOKENIZED_HISTORY_BLOCK_SIZE', db)} tokens (approx)")
    print(f"  - Recent History: up to {TOKENIZE_THRESHOLD} tokens")
    print(f"  - TOTAL MEMORY BUDGET: ~{total_memory_tokens} tokens")
    print(f"{'='*80}\n")

@app.delete("/history/{history_id}", response_model=dict)
async def delete_history_entry(
    history_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    history_entry = db.query(StoryHistory).filter(StoryHistory.id == history_id).first()
    if not history_entry:
        raise HTTPException(status_code=404, detail="History entry not found")
    game = db.query(SavedGame).filter(SavedGame.id == history_entry.saved_game_id).first()
    if not game:
        raise HTTPException(status_code=404, detail="Saved game not found")
    if game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your saved game")
    
    # Check for tokenized chunks that reference this history entry
    tokenized_chunks = db.query(TokenizedHistory).filter(
        TokenizedHistory.saved_game_id == history_entry.saved_game_id
    ).all()
    
    for chunk in tokenized_chunks:
        if chunk.history_references:
            ref_ids = [int(id.strip()) for id in chunk.history_references.split(',')]
            if history_id in ref_ids:
                # Remove this ID from references
                ref_ids.remove(history_id)
                
                if not ref_ids:
                    # No more references - delete the tokenized chunk
                    db.delete(chunk)
                else:
                    # Update the references list
                    chunk.history_references = ','.join(str(id) for id in ref_ids)
    
    db.delete(history_entry)
    db.commit()
    return {"detail": "History entry deleted"}

@app.put("/history/{history_id}", response_model=HistoryDTO)
async def update_history_entry(
    history_id: int,
    update_data: dict,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    history_entry = db.query(StoryHistory).filter(StoryHistory.id == history_id).first()
    if not history_entry:
        raise HTTPException(status_code=404, detail="History entry not found")
    game = db.query(SavedGame).filter(SavedGame.id == history_entry.saved_game_id).first()
    if not game:
        raise HTTPException(status_code=404, detail="Saved game not found")
    if game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your saved game")
    
    # Update the text field (model uses 'text', not 'entry')
    if "text" in update_data:
        history_entry.text = update_data["text"]
        # Recalculate token count for the modified entry
        history_entry.token_count = count_tokens_batch([history_entry.text])[0]
    
    db.commit()
    db.refresh(history_entry)
    return HistoryDTO.model_validate(history_entry)

@app.post("/tokenized_history/", response_model=TokenizedHistoryDTO, status_code=201)
async def create_tokenized_history_entry(
    th_data: TokenizedHistoryIn,
    saved_game_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    game = db.query(SavedGame).filter(SavedGame.id == saved_game_id).first()
    if not game or game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your saved game")
    new_th = TokenizedHistory(
        saved_game_id=saved_game_id,
        start_index=th_data.start_index,
        end_index=th_data.end_index,
        summary=th_data.summary,
        created_at=datetime.now(timezone.utc)
    )
    db.add(new_th)
    db.commit()
    db.refresh(new_th)
    return tokenized_history_to_dto(new_th)

@app.put("/tokenized_history/{tokenized_id}", response_model=TokenizedHistoryDTO)
async def update_tokenized_history_entry(
    tokenized_id: int,
    update_data: dict,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    tokenized_entry = db.query(TokenizedHistory).filter(TokenizedHistory.id == tokenized_id).first()
    if not tokenized_entry:
        raise HTTPException(status_code=404, detail="Tokenized history entry not found")
    game = db.query(SavedGame).filter(SavedGame.id == tokenized_entry.saved_game_id).first()
    if not game:
        raise HTTPException(status_code=404, detail="Saved game not found")
    if game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your saved game")
    
    # Update allowed fields
    if "summary" in update_data:
        tokenized_entry.summary = update_data["summary"]
        # Recalculate token count for the modified summary
        tokenized_entry.token_count = count_tokens_batch([tokenized_entry.summary])[0]
    if "start_index" in update_data:
        tokenized_entry.start_index = update_data["start_index"]
    if "end_index" in update_data:
        tokenized_entry.end_index = update_data["end_index"]
    
    db.commit()
    db.refresh(tokenized_entry)
    return tokenized_history_to_dto(tokenized_entry)

@app.delete("/tokenized_history/{tokenized_id}")
async def delete_tokenized_history_entry(
    tokenized_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    tokenized_entry = db.query(TokenizedHistory).filter(TokenizedHistory.id == tokenized_id).first()
    if not tokenized_entry:
        raise HTTPException(status_code=404, detail="Tokenized history entry not found")
    game = db.query(SavedGame).filter(SavedGame.id == tokenized_entry.saved_game_id).first()
    if not game:
        raise HTTPException(status_code=404, detail="Saved game not found")
    if game.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Forbidden: not your saved game")
    
    db.delete(tokenized_entry)
    db.commit()
    return {"detail": "Tokenized history entry deleted"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)